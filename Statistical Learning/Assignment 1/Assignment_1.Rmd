---
title: "MATH5743M: Statistical Learning: Assessed Practical 1 - Predicting the Olympic Games"
author: "Anupam Bose"
date: "Semester 2 2022"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Predicting the Olympic Games

We will investigate the medals won by 71 countries in year 2008 in Beijing, 2012 in London and 2016 in in Rio. The dataset contains countries who have won at least one gold medal in each of the last three games. It contains data of their population, GDP (in billions of US dollars) and number of medals won in those years.

We will use multivariate regression method to predict number of medals won by training the model using various methods.
 
## Task 1:

The libraries to import to do further analysis on data
```{r}
library(Metrics)
library(MASS)
library(tidyverse)
library(sf)
library(tinytex)
```

The data has been imported into a dataframe (df) by using the **read.csv** function.
```{r}
df = read.csv("medal_pop_gdp_data_statlearn.csv")
```

The first 6 rows of our data will give us some idea about it.
```{r}
head(df)
```

Now, let's look into summary statistics of our data:
```{r}
summary(df)
```

Linear models are a type of model that describes a response variable as a linear combination of predictor variables.Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable). The variables we are using to predict the value of the dependent variable are called the independent variables (or sometimes, the predictor, explanatory or regressor variables).

The multiple linear regression for Y as a function
of X is given by the following equation:
$$Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon, \ \epsilon \sim N(0, \sigma^2)$$
where,
\begin{itemize}
\item $x_1$ and $x_2$ are input variables
\item A scalar constant – $\beta_0$
\item $\beta_1$, $\beta_2$ are regression coefficients
\item A residual $\epsilon$ is unknown, but assumed to be normally distributed with zero mean and unknown variance:$$\epsilon \sim(0, \sigma^2)$$
\item $Y$ is the target variable
\end{itemize}

In R, we will use glm() function to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution.
Generalized linear model (GLM) is a generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution like Gaussian distribution.

```{r}
model_train = glm(Medal2012 ~ Population + GDP , data= df)
summary(model_train)
```

Here we have considered two predictor variables which is population and GDP. Medal2012 is our target variable.
The key points to note from the summary are:

\begin{itemize}
\item The regression coefficient of the predictor variable population is $5.247 \times 10^{-9}$ which is very small with the standard error of $7.193 \times 10^{-9}$. The P-value is 0.468 which is quite large$(0.4682 \gg 0.05)$. This shows that the population of the country is statistically insignificant with regards to country's medal count.
\item On the other hand, the regression coefficient of the predictor variable GDP is $7.564 \times 10^{−3}$ which is relatively large with the standard error of $7.325 \times e^{-4}$. The P-value is of it is $1.45 \times 10^{-15}$ which is quite small and is less than 0.05$(1.45 \times 10^{-15} \ll 0.05)$. It tells that the GDP of a country is statistically significant and does impact country's medal count.
\item The intercept is $6.076$.
\end{itemize}

To confirm the significance of the predictor variables we calculate confidence interval of their coefficient which is given by:
$$\textrm{C.I.: Estimate} \pm t_c \times \textrm{Standard Error}$$

The t-statistic value$(t_c)$ can be calculated by using $qt$ function. We have 71 data points with one intercept and two regression coefficients. Therefore, the number of degrees of freedom will be 71-3 = 68. With $95\%$ confidence interval for which $P(t > t_c) = 0.975$, $t_c$ is calculated. 
```{r}
#t-statistic Value 
tc = qt(p=0.975, df=68)

#Confidence Interval of population
pop_ci = summary(model_train)$coefficients[2, 1] +
  c(-1,1)*tc*summary(model_train)$coefficients[2, 2]
print(pop_ci)

#Confidence Interval of gdp
gdp_ci = summary(model_train)$coefficients[3, 1] +
  c(-1,1)*tc*summary(model_train)$coefficients[3, 2]
print(gdp_ci)
```
The following observation is made from the confidence intervals:
\begin{itemize}
\item The confidence interval of the coefficient of the variable population$(\beta_1)$ shows that $0$ lies in its range. This confirms the insignificance of the variable with regards to medal count. when $\beta_1=0$, it has no impact on the target variable.
\item The confidence interval of the coefficient of the variable gdp$(\beta_2)$ is positive and $0$ does not lie in its range. The values are quite small but it shows some significance. Hence, it does impact our target variable relatively. 
\end{itemize}
 

Considering no change in country's GDP and population, the trained model can now be used to predict medal count for the year 2016(Medal2016) using predict function. The first 10 predicted vs observed values of medal count has been shown below.
```{r}
#Testing model with prediction
model_test=df[,c(2,3)]
pred= predict(model_train, newdata = model_test)
df %>%
  mutate(Pred_Medal2016=round(pred)) %>%
  select(Medal2016,Pred_Medal2016)%>%
  head(10)
```
The plot of predicted vs observed value of Medal2016 gives better insights. The regression line is also fitted in the plot. To see the data points better the axes are log transformed. The plot shows that the data points are quite away from the regression line. The model isn't performing good but decently.


```{r}
#Plot with log transformed axes
ggplot(data=df, aes(x=pred, y=Medal2016,label = Country)) +
  geom_point() +
  ggtitle("Observed VS Predicted Medal Count for 2016")+
  scale_y_continuous(trans='log10')+
  scale_x_continuous(trans='log10')+
  xlab("Predicted Medal Count for 2016")+
  ylab("Observed Medal Count for 2016")+
  geom_abline(slope = 1, intercept = 0, col = 'red')+
  geom_text(size=1.5,nudge_y = 0.05,  check_overlap = TRUE)
```

Boxplot of the absolute difference between the predicted and observed values shows 4 outliers. It can be known by finding the 75th quantile and Interquantile range of it using function quantile() 
```{r}
#Boxplot of Absolute error 
boxplot(abs(pred - df$Medal2016))

#Quantile
quantile(abs(pred - df$Medal2016))

#Outliers of Absolute errors: 
#Values Beyond [75th Quantile +(1.5*IQR)]

6.61 + 1.5*IQR(abs(pred - df$Medal2016))  

```
Hence, following 4 countries are outliers with Great Britain having the highest absolute error:
```{r}
#Countries with Absolute Error Outliers
df %>% 
  select(Country,Medal2016) %>%
  mutate(Absolute_Error = abs(pred - Medal2016)) %>%
  mutate(pred=round(pred)) %>%
  select(Country,Medal2016,pred,Absolute_Error) %>%
  mutate(Absolute_Error=round(Absolute_Error)) %>%
  filter(Absolute_Error>12.786)%>%
  arrange(desc(Absolute_Error))
```
The model's accuracy can be determined by finding mean absolute error using function $mae()$ and Root mean squared error using function $rmse()$.

```{r}
#Mean Absolute Error and Root Mean Squared Error
mae(df$Medal2016,pred)
rmse(df$Medal2016,pred)

```
The value of mean absolute error and RMSE is $6.1043$ and $9.11$ respectively which shows the model is performing decently. 

## Task 2:

We repeat the task $1$ but with log-transformed medal count in 2012 to check and compare the performance of the model with the previous one.

The benefits of the logarithmic transformation are:
\begin{itemize}
\item Reduces overfitting of data.
\item Less computational power is required.
\item Helps if the distribution is skewed by transforming it. 
\item Improves linearity between predictor and target variables.
\end{itemize}


```{r}
log_of_Medal2012=log(df$Medal2012)
log_model_train = glm(log_of_Medal2012 ~ Population + GDP , data= df)
summary(log_model_train)
```
The following observations could be made from the model:
\begin{itemize}
\item The regression coefficient of the predictor variable population is $1.105 \times 10^{-10}$ which is very small with the standard error of $6.058 \times 10^{-10}$. The P-value is $0.856$ which is quite large $(0.856 \gg 0.05)$. This shows that the population of the country is statistically insignificant with regards to country's medal count.
\item On the other hand, the regression coefficient of the predictor variable GDP is $3.161 \times 10^{−4}$ which is relatively large with the standard error of $6.170 \times 10^{-5}$. The P-value is of it is $2.68 \times 10^{-6}$ which is quite small and is less than 0.05$(2.68 \times 10^{-6} \ll 0.05)$. It tells that the GDP of a country is statistically significant and does impact country's medal count.
\item The intercept is $1.569$.
\end{itemize}

```{r}
tc <- qt(p=0.975, df=68)
#Confidence Interval of population
pop_ci_log <- summary(log_model_train)$coefficients[2, 1] +
  c(-1,1)*tc*summary(log_model_train)$coefficients[2, 2]
print(pop_ci_log)
#Confidence Interval of gdp
gdp_ci_log = summary(log_model_train)$coefficients[3, 1] +
  c(-1,1)*tc*summary(log_model_train)$coefficients[3, 2]
print(gdp_ci_log)
```


The following observation is made from the confidence intervals:
\begin{itemize}
\item The confidence interval of the coefficient of the variable population$(\beta_1)$ shows that $0$ lies in its range. This confirms the insignificance of the variable with regards to medal count. when $\beta_1=0$, it has no impact on the target variable.
\item The confidence interval of the coefficient of the variable gdp$(\beta_2)$ is positive and $0$ does not lie in its range. The values are quite small but it shows some significance. Hence, it does impact our target variable relatively. 
\end{itemize}

Considering no change in country's GDP and population, the trained model can now be used to predict medal count for the year 2016(Medal2016) using predict function. The first 10 predicted vs observed values of medal count has been shown below.

```{r}
log_df <- df[,c(2,3)]
log_predictions <- exp(predict(log_model_train, newdata = log_df))
df %>%
  mutate(Pred_Medal2016=round(log_predictions)) %>%
  select(Medal2016,Pred_Medal2016)%>%
  head(10)
```
The plot of predicted vs observed value of Medal2016 gives better insights. The regression line is also fitted in the plot. To see the data points better the axes are log transformed. The plot shows that the data points are again away from the regression line. The performance of the model has not improved compared to the previous one. Hence, the log transformation of the target variable does not improve the model.

```{r}
#Plot with log transformed axes
ggplot(data=df, aes(x=log_predictions, y=Medal2016,label = Country)) +
  geom_point() +
  ggtitle("Observed VS Predicted Medal Count for 2016")+
  scale_y_continuous(trans='log10')+
  scale_x_continuous(trans='log10')+
  xlab("Predicted Medal Count for 2016")+
  ylab("Observed Medal Count for 2016")+
  geom_abline(slope = 1, intercept = 0, col = 'red')+
  geom_text(size=1.5,nudge_y = 0.05,  check_overlap = TRUE)

```
We again find Outliers by calculating 75th quantile and interquantile range. This time we get 6 outliers with United States having the highest absolute error:
```{r}
#Quantile
quantile(abs(log_predictions - df$Medal2016))

#Outliers of Absolute errors: 
#Values Beyond [75th Quantile +(1.5*IQR)]
8.76 + 1.5*IQR(abs(log_predictions - df$Medal2016))  


#Countries with Absolute Error Outliers
df %>% 
  select(Country,Medal2016) %>%
  mutate(Absolute_Error = abs(log_predictions - Medal2016)) %>%
  mutate(log_predictions=round(log_predictions)) %>%
  select(Country,Medal2016,log_predictions,Absolute_Error) %>%
  mutate(Absolute_Error=round(Absolute_Error)) %>%
  filter(Absolute_Error>18.619)%>%
  arrange(desc(Absolute_Error))
```
Mean absolute error and RMSE of the model:

```{r}
#Mean Absolute Error and Root Mean Squared Error
mae(df$Medal2016,pred)
rmse(df$Medal2016,pred)

```
The value of mean absolute error and RMSE is $13.70$ and $56.62$ respectively which shows the model isn't performing better than the previous one. 

## Task 3:
We repeat the task $1$ by assuming $Medal2016$ has Poisson distribution and compare the performance of the model with the previous two models.  

The reasons why Poisson Regression model can be considered :
\begin{itemize}
\item The model works best if the data is discrete with non-negative integer values.
\item The outcome are counts of events and occuring randomly  at constant rate.
\item Medals obtained are discrete values with some counts.
\end{itemize}

```{r}
Poisson_Model = glm(Medal2012 ~ Population + GDP , data= df,family = poisson(link='log'))
summary(Poisson_Model)
```
The following observations could be made from the model:
\begin{itemize}
\item The regression coefficient of the predictor variable population is $6.049 \times 10^{-10}$ which is very small with the standard error of $9.131 \times 10^{-11}$. The P-value is $3.48 \times 10^{-11}$ which is quite small$(3.48 \times 10^{-11} \ll 0.05)$. This shows that the population of the country is statistically significant with regards to country's medal count.
\item Also, the regression coefficient of the predictor variable GDP is $1.715 \times 10^{−4}$ which is relatively large with the standard error of $6.672 \times 10^{-6}$. The P-value is of it is less than $2 \times 10^{-16}$ which is quite small and is less than 0.05$(2 \times 10^{-16} \ll 0.05)$. It tells that the GDP of a country is statistically significant and does impact country's medal count.
\item The intercept is $2.193$.
\end{itemize}

```{r}
tc = qt(p=0.975, df=68)
#Confidence Interval of population
pop_ci_poi = summary(Poisson_Model)$coefficients[2, 1] +
  c(-1,1)*tc*summary(Poisson_Model)$coefficients[2, 2]
print(pop_ci_poi)
#Confidence Interval of gdp
gdp_ci_poi = summary(Poisson_Model)$coefficients[3, 1] +
  c(-1,1)*tc*summary(Poisson_Model)$coefficients[3, 2]
print(gdp_ci_poi)
```

The following observation is made from the confidence intervals:
\begin{itemize}
\item The confidence interval of the coefficient of the variable population$(\beta_1)$ is positive and $0$ does not lie in its range.The values are extremely small but it does shows some significance. This confirms the significance of the variable with regards to medal count. 
\item The confidence interval of the coefficient of the variable gdp$(\beta_2)$ is positive and again $0$ does not lie in its range. The values are quite small but it shows some significance. Hence, it does impact our target variable. The values also confirms that GDP is more significant than population.  
\end{itemize}

Considering no change in country's GDP and population, the trained model can now be used to predict medal count for the year 2016(Medal2016) using predict function. The first 10 predicted vs observed values of medal count has been shown below.

```{r}
#Testing model with prediction
poi_df <- df[,c(2,3)]
poi_predictions= predict(Poisson_Model, newdata = poi_df, type = "response")
df %>%
  mutate(Pred_Medal2016=round(poi_predictions)) %>%
  select(Medal2016,Pred_Medal2016)%>%
  head(10)
```
The plot of predicted vs observed value of Medal2016 will again give better insights. The regression line is also fitted in the plot. To see the data points better the axes are log transformed.

The plot shows that the data points are again quite away from the regression line. There are very few data points which are lying on it. The model is performing quite poorly. The slope of the regression line is also more than the previous models.

```{r}
#Plot with log transformed axes
ggplot(data=df, aes(x=poi_predictions, y=Medal2016,label = Country)) +
  geom_point() +
  ggtitle("Observed VS Predicted Medal Count for 2016")+
  scale_y_continuous(trans='log10')+
  scale_x_continuous(trans='log10')+
  xlab("Predicted Medal Count for 2016")+
  ylab("Observed Medal Count for 2016")+
  geom_abline(slope = 1, intercept = 0, col = 'red')+
  geom_text(size=1.5,nudge_y = 0.05,  check_overlap = TRUE)

```
We again find outliers by calculating 75th quantile and interquantile range.

we have 7 outliers with Great Britain having the highest absolute error:

```{r}
#Quantile
quantile(abs(poi_predictions - df$Medal2016))

#Outliers of Absolute errors: 
#Values Beyond [75th Quantile +(1.5*IQR)]

8.33 + 1.5*IQR(abs(poi_predictions - df$Medal2016))  


#Countries with Absolute Error Outliers
df %>% 
  select(Country,Medal2016) %>%
  mutate(Absolute_Error = abs(poi_predictions - Medal2016)) %>%
  mutate(poi_predictions=round(poi_predictions)) %>%
  select(Country,Medal2016,poi_predictions,Absolute_Error) %>%
  filter(Absolute_Error>15.93)%>%
  arrange(desc(Absolute_Error))

```
Mean absolute error and RMSE of the model:

```{r}
#Mean Absolute Error and Root Mean Squared Error
mae(df$Medal2016,poi_predictions)
rmse(df$Medal2016,poi_predictions)

```
The value of mean absolute error and RMSE is $7.85$ and $11.80$ respectively.



